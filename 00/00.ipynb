{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduction to tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### you can refer to the documentation for tensor class present in pytorch module : https://pytorch.org/docs/stable/tensors.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = torch.tensor(21)\n",
    "scalar # will return the type as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item will provide the value present in the tensor while ndim is for dimension\n",
    "#item onnly woks with the scalar\n",
    "print(scalar.item())\n",
    "print(scalar.ndim)\n",
    "print(scalar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = torch.tensor([21,27])\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### here dimension will return the number of close square brackets while shape provides the info about the total elements present in the tensor and brackets define how elements are structred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector.ndim)\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX = torch.tensor([[1,2],[3,4]])\n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MATRIX.ndim)\n",
    "print(MATRIX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR = torch.tensor([[[1,2,3],[4,5,6],[7,8,9]]])\n",
    "print(TENSOR)\n",
    "print(TENSOR.ndim)\n",
    "print(TENSOR.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why random tensors?\n",
    "\n",
    "Random tensors are important because the way many neural network learn is that they start with tensors full of random numbers and then adjust those random numbers to better represent the data\n",
    "\n",
    "`Start with random numbers -> look at data -> update random numbers ->look at data -> update random numbers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random tensors of size (3,4)\n",
    "\n",
    "random_tensor = torch.rand(3,4)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random tensor with similar shape to as image tensor\n",
    "tensor_with_image_shape = torch.rand(size=(3,28,28)) # color chennel, height, width ,size parameter is optional\n",
    "tensor_with_image_shape,tensor_with_image_shape.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeroes and Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor containing all zeroes and also used as mask\n",
    "zeros = torch.zeros(size=(3,4))\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask using\n",
    "rt = torch.rand(size=(3,4))\n",
    "ft = rt*zeros\n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(size=(4,5))\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zeros.dtype)\n",
    "print(ones.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tensor range and tensor like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_with_range = torch.arange(start=1,end=11,step=2)\n",
    "tensor_with_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor like\n",
    "new_tensor = torch.ones_like(input=tensor_with_range) #zeros like\n",
    "new_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b> : Tensor datatypes is one of the 3 big errors you'll run into with pytorch and deep learning\n",
    "\n",
    "<ol>\n",
    "<li>Tensors not right datatype</li>\n",
    "<li>Tensors not right shape</li>\n",
    "<li>Tensors not on the right device</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#float 32 tensor\n",
    "# by default the datatype of each tensor created is float32\n",
    "#32 and 16 is about precision\n",
    "float_32_tensor = torch.tensor([1.0,2.0,3.0],\n",
    "                               dtype=None, # what datatype is the tensor (e.g. float32 or float16)\n",
    "                               device=None, # what device is your tensor on\n",
    "                               requires_grad=False # whether or not to track gradients with this tensors operations\n",
    "                               ) \n",
    "float_32_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_16_tensor = float_32_tensor.type(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_16_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_16_tensor*float_32_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting information from tensors\n",
    "<ol>\n",
    "<li>Tensors not right datatype - to do get datatype from a tensor, can use `tensor.dtype`</li>\n",
    "<li>Tensors not right shape - to get shape from a tensor, can use `tensor.shape`</li>\n",
    "<li>Tensors not on the right device - to get device from a tensor, can use `tensor.device`</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tensor = torch.tensor([1,2,3])\n",
    "print(some_tensor)\n",
    "print(f\"datatype of the tensor is {some_tensor.dtype}\")\n",
    "print(f\"the device on which the tensor is : {some_tensor.device}\")\n",
    "print(f\"the shape of the tensor is : {some_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating tensors (tensor operation)\n",
    "\n",
    "Tensor operation includes:\n",
    "<ul>\n",
    "<li>Addition</li>\n",
    "<li>Subtraction</li>\n",
    "<li>Multiplication (element wise)</li>\n",
    "<li>Division</li>\n",
    "<li>Matrix multiplication</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addition, subtraction, multiplication, division\n",
    "tensor = torch.tensor([1,2,3])\n",
    "tensor = tensor - 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using in built function present in torch library mul,sub,mul,div \n",
    "tensor = torch.sub(tensor,5)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "two main ways of performing multiplication in neural networks and deep learning \n",
    "1. Element wise multiplication\n",
    "2. Matrix multiplication (dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor =  torch.tensor([1,2,3])\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1,2,3])\n",
    "t2 = torch.tensor([4,5,6])\n",
    "print(f\"normal multiplication : {t1*t2}\")\n",
    "print(f\"with matrix multiplication : {torch.matmul(t1,t2)}\") # it takes very less time to execute with comparison of normal multiplication\n",
    "#here writing mm would also be fine it's an alias of matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green\">when two matrix are multiplied the main condition is that the inner dimension of them should be equal and resultant matrix will be of outer dimension.\n",
    "that is why the concept of transposing a matrix will be in picture.</p>\n",
    "Just visualize,visualize,visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_a = torch.rand(size=(2,3))\n",
    "tensor_b = torch.rand(size=(2,3))\n",
    "print(tensor_a)\n",
    "print(tensor_b)\n",
    "print(tensor_b.T)\n",
    "print(torch.mm(tensor_a,tensor_b.T))\n",
    "\n",
    "print(tensor_a.shape)\n",
    "print(tensor_b.shape)\n",
    "print(tensor_b.T.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding the min,max,mean,sum, etc (tensor aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a tensor\n",
    "t = torch.arange(start=1,end=50,step=5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(t),torch.max(t)\n",
    "#or\n",
    "t.min(),t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here torch.mean() function requires a tensor of float32 datatype to work but the datatype of \"t\" is Long here\n",
    "# t.mean()\n",
    "t.type(torch.float32).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the positional min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(start=1,end=50,step=5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.argmax(),t.argmin()\n",
    "#or\n",
    "torch.argmax(t),torch.argmin(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping, stacking, squeezing and unsqueezing tensors\n",
    "\n",
    "<ul>\n",
    "<li><b>Reshaping</b> reshapes an input tensor to a defined shape</li>\n",
    "<li><b>View</b> Returns a view of an input tensor of certain shape but keep the same memory as the original tensor</li>\n",
    "<li><b>Stacking</b> combine multiple tensors on top of each other (vstack) or side by side (hstack)</li>\n",
    "<li><b>Squeeze</b> removes all <code>1</code> dimensions from a tensor</li>\n",
    "<li><b>Unsqueeze</b> add a <code>1</code> dimension to a target tensor</li>\n",
    "<li><b>Permute</b> return a view of the input with dimensions permuted (swapped) in a certain way</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.arange(1,10,1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping takes place only when the multiplication of reshaping dimension equals to number of elements \n",
    "print(x.shape)\n",
    "x_reshaped = x.reshape(3,3) #possibilities (1,9),(9,1)\n",
    "print(x_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the view\n",
    "z = x.view(3,3)\n",
    "print(z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes z changes x (because a view of a tensor shares the same memory as the original input)\n",
    "z[1,1]=3\n",
    "print(z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.squeeze() - removes all single dimension from a target tensor\n",
    "t = torch.rand(size=(3,4))\n",
    "print(t)\n",
    "t = t.reshape(1,3,4)\n",
    "print(\"t reshaped : \",t)\n",
    "\n",
    "print(t.shape)\n",
    "t = t.squeeze()\n",
    "print(f\"after applying the squeeze function shape of tensor would be : {t.shape}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.unsqueeze() - adds a single dimension to a target tensor at a specific dim\n",
    "print(f\"previous target : {t}\")\n",
    "print(f\"previous sahpe : {t.shape}\")\n",
    "\n",
    "#add an extra dimension with unsqueeze\n",
    "t = t.unsqueeze(dim=2)\n",
    "print(f\"\\nnew tensor : {t}\")\n",
    "print(f\"new tensor shape : {t.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.permute - rearranges the dimensions of a target tensor in a specified order \n",
    "#image matrix would be the best example here\n",
    "\n",
    "image = torch.rand(size=(224,224,3))\n",
    "print(image.size())\n",
    "\n",
    "image_permuted = torch.permute(image,(2,0,1)) # it indicates the dimension present in main tensor\n",
    "print(image_permuted.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0,0,0] = 2127\n",
    "image_permuted[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stacked = torch.stack([x,x,x,x],dim=0)\n",
    "print(x_stacked)\n",
    "x_stacked = torch.stack([x,x,x,x],dim=1)\n",
    "print(x_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stacked = torch.hstack([x,x,x,x]) #stacking horizontally will take all the element in single dimension of single tensor\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing (selecting data from tensors)\n",
    "\n",
    "#### Indexing with PyTorch is similar ro indexing with NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.arange(1,10).reshape(1,3,3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's index on our new tensor\n",
    "print(x[0])\n",
    "print(x[0][0])   # can also be written as [0,0]\n",
    "print(x[0][0][0].item()) # [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x[0])):\n",
    "    print(x[:,i,i].item(),end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch tensors & NumPy\n",
    "##### Numpy is a popular scentific Python numeric computing library.\n",
    "##### And because of this, PyTorch  has functionality to interect with it.\n",
    "\n",
    "<ul>\n",
    "<li>Data in NumPy, want in PyTorch tensor -> <code>torch.from_numpy(ndarray)</code></li>\n",
    "<li>PyTorch tensor -> NumPy-> <code>torch.Tensor.numpy()</code></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy array to tensor\n",
    "array = np.arange(1.0,8.0)\n",
    "tensor = torch.from_numpy(array) # here pytorch reflects numpy's default datatype unless specified  \n",
    " \n",
    "array,tensor\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array.dtype,tensor.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : numpy_array->float64, tensor->float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(1.0,8.0).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the value of array what will this do to tensor and same process will be done when change the value of tensor and nothing will be done to array\n",
    "array = array + 1\n",
    "array,tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor to numpy array\n",
    "tensor = torch.ones(7)\n",
    "numpy_tensor = tensor.numpy()\n",
    "tensor,numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility (trying to take random out of random)\n",
    "\n",
    "In short how a neural network learns: start with random number -> tensor operations -> update random numbers to try and make them better representarions of the data -> again -> again -> again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(size=(3,4))\n",
    "t2 = torch.rand(size=(3,4))\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t1 == t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual seed will create random but reproducable tensors \n",
    "#agenda of random seed is to use same random numbers everytime while executing the code \n",
    "#f you want same random number in new defined tensor then you have to write code(torch.manual_seed()) before defining a new t ensor \n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "t1 = torch.rand(size=(3,4))\n",
    "torch.manual_seed(random_seed)\n",
    "t2 = torch.rand(size=(3,4))\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t1 == t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUs - makes computing faster\n",
    "fast computation on numbers, thanks to CUDA + NVIDIA hardware + pytorch working behind the scene to make everything hunky dory(good).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for gpu access with pytorch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup device agnostic code \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "#cuda is computation toolkit provided by nvidia, it is for fast computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of gpus\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking tansor OR model to gpu\n",
    "# define a device, define a tensor, and take tensor to the device defined \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tensor = torch.rand(size=(1,5))\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "\n",
    "# another simple method of checking weather the tensor is on gpu or cpu is that you can convert tensor to numpy array \n",
    "# If tensor is on GPU, can't transform it to numpy\n",
    "# from that point you can move your tensor to cpu\n",
    "tensor_on_cpu = tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>tensor_on_gpu.numpy() will lead to typeError if tensor_on_gpu is on gpu</code>\n",
    "\n",
    "To fix the GPU tensor with numpy issue, we can first set it to the CPU\n",
    "\n",
    "<code>tensor_on_cpu = tensor_on_gpu.cpu()</code>\n",
    "\n",
    "<code>tensor_on_cpu.numpy()</code> will work fine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "#create linear regression model class\n",
    "#every model in pytorch inherit from nn.module that's why you need to define forward method for overriding purpose\n",
    "\n",
    "\n",
    "class LinearRegressionModel(nn.Module): # almost everything in pytorch inherits from nn.Module\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weights = nn.Parameter(torch.randn(1, # <- start with random weight and try to adjust it to the ideal weight\n",
    "                                                requires_grad=True, # <- can this parameter be updated via gradient descent?\n",
    "                                                dtype=torch.float)) # <- PyTorch loves the datatype torch.float32\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias and try to adjust it to the ideal bias\n",
    "                                             requires_grad=True, # <- can this parameter be updated via gradient descent?\n",
    "                                             dtype=torch.float)) # <- PyTorch loves the datatype torch.float32\n",
    "        \n",
    "        #Forward method to define the computation in the model \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x is input data\n",
    "        return self.weights*x + self.bias # this is the linear regression formula\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
